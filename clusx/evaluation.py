"""
Evaluation module for clustering quality assessment.

This module provides tools for evaluating the quality and characteristics of clusters
generated by Bayesian nonparametric clustering algorithms. It implements established
metrics for cluster validation in the context of text data clustering, with a focus
on power-law analysis and similarity-based metrics.

Key components:

- ClusterEvaluator: Main class for evaluating clustering results
- NumpyEncoder: Custom JSON encoder for handling NumPy data types
- save_evaluation_report: Function to save evaluation results to JSON

The evaluation process assesses:

1. Cluster cohesion and separation (silhouette score)
2. Intra-cluster vs. inter-cluster similarity
3. Power-law characteristics of cluster size distributions
4. Potential outliers in the clustering results

This module is typically used after running clustering with the Dirichlet Process
and Pitman-Yor Process models to compare their performance and understand the
statistical properties of the generated clusters.
"""

from __future__ import annotations

import json
import os
from typing import TYPE_CHECKING

import numpy as np
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors

if TYPE_CHECKING:
    import numpy
    from typing import Any, Union

from .logging import get_logger

logger = get_logger(__name__)


class NumpyEncoder(json.JSONEncoder):
    """
    Custom JSON encoder that handles NumPy data types.

    This encoder converts NumPy types to their Python equivalents for proper JSON
    serialization. It's used when saving evaluation reports to ensure all NumPy
    values are properly converted to standard Python types.

    Conversions:

    - :class:`numpy.ndarray` → :class:`list`
    - :class:`numpy.single`, :class:`numpy.double` → :class:`float`
    - :class:`numpy.intc`, :class:`numpy.int_` → :class:`int`
    - :class:`numpy.bool_` → :class:`bool`
    - Other NumPy types → Python equivalents via the `item()` method when available
    """

    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.single, np.double)):  # type: ignore
            return float(obj)
        if isinstance(obj, (np.intc, np.int_)):  # type: ignore
            return int(obj)
        if isinstance(obj, np.bool):  # type: ignore
            return bool(obj)
        if isinstance(obj, bool):
            return bool(obj)
        try:
            # Try to convert any other NumPy type to a Python type
            return obj.item() if hasattr(obj, "item") else obj
        except (AttributeError, ValueError, TypeError):
            return super().default(obj)


class ClusterEvaluator:
    """
    Evaluates the quality and characteristics of text clusters using metrics.

    This class provides methods to assess clustering results through various metrics:

    - Silhouette Score: Measures how similar an object is to its own cluster
      compared to other clusters
    - Similarity Metrics: Evaluates intra-cluster vs inter-cluster similarity
    - Power-law Analysis: Determines if cluster sizes follow a power-law distribution
    - Outlier Detection: Identifies potential outliers in the clustering results

    Used for post-processing analysis of Bayesian nonparametric clustering results.

    Note:
        Parameters like alpha and d in clustering algorithms significantly impact
        the resulting cluster distributions.
    """

    def __init__(
        self,
        texts: list[str],
        embeddings: numpy.ndarray,
        cluster_assignments: list[int],
        model_name: str,
        alpha: float = 1.0,
        sigma: float = 0.0,
        variance: float = 0.1,
        random_state: "Union[int, None]" = None,
    ):
        """
        Initialize the cluster evaluator.

        Args:
            texts: List of text strings that were clustered
            embeddings: Numpy array of embeddings for each text
            cluster_assignments: List of cluster IDs for each text
            model_name: Name of the clustering model (e.g., "DP", "PYP")
            alpha: Concentration parameter (default: 1.0)
            sigma: Discount parameter for Pitman-Yor Process (default: 0.0)
            variance: Variance parameter for likelihood model (default: 0.1)
            random_state: Random seed for reproducible evaluation (default: None)
        """
        self.texts = texts
        self.embeddings = embeddings
        self.cluster_assignments = cluster_assignments
        self.model_name = model_name
        self.alpha = alpha
        self.sigma = sigma
        self.variance = variance
        self.random_state = random_state
        self.unique_clusters = sorted(set(cluster_assignments))

        # Set random state for reproducibility if provided
        if random_state is not None:
            np.random.seed(random_state)

        # Validate inputs
        if len(texts) != len(embeddings) or len(texts) != len(cluster_assignments):
            raise ValueError(
                "Length mismatch: texts, embeddings, and cluster_assignments "
                "must have the same length"
            )

        logger.info(
            f"Initialized cluster evaluator for {model_name} with {len(texts)} texts "
            f"and {len(self.unique_clusters)} clusters"
        )

    def calculate_silhouette_score(self) -> Union[float, int]:
        """
        Calculate the silhouette score for the clustering.

        The silhouette score measures how similar an object is to its own cluster
        compared to other clusters. The score ranges from -1 to 1, where:

        - A high value (close to 1) indicates the object is well-matched to its cluster
        - A value near 0 indicates the object is on or very close to the decision
          boundary
        - A negative value indicates the object might be assigned to the wrong cluster

        This method handles edge cases:

        - Returns 0.0 if there are fewer than 2 clusters
        - Returns 0.0 if any cluster has fewer than 2 samples
        - Uses cosine distance for text embeddings

        Returns:
            float: Silhouette score as a float between -1 and 1, or 0.0 if calculation
                  is not possible
        """
        # We need at least 2 clusters and each cluster must have at least 2 samples
        if len(self.unique_clusters) < 2:
            logger.warning(
                f"Cannot calculate silhouette score: only "
                f"{len(self.unique_clusters)} cluster found"
            )
            return 0.0

        # Count samples per cluster
        cluster_counts = {}
        for cluster_id in self.cluster_assignments:
            cluster_counts[cluster_id] = cluster_counts.get(cluster_id, 0) + 1

        # Check if any cluster has only one sample
        single_sample_clusters = [c for c, count in cluster_counts.items() if count < 2]
        if single_sample_clusters:
            logger.warning(
                f"Cannot calculate silhouette score: "
                f"{len(single_sample_clusters)} clusters have fewer than 2 samples"
            )
            return 0.0

        try:
            score = silhouette_score(
                self.embeddings, self.cluster_assignments, metric="cosine"
            )
            logger.info(f"Silhouette score for {self.model_name}: {score:.4f}")
            return float(score)
        except Exception as e:
            logger.error(f"Error calculating silhouette score: {e}")
            return 0.0

    def calculate_similarity_metrics(self) -> dict[str, Union[float, numpy.floating]]:
        """
        Calculate similarity metrics for the clusters.

        This method computes three key metrics using cosine similarity:

        - Intra-cluster similarity:
          Average similarity between texts in the same cluster (higher values indicate
          more cohesive clusters)
        - Inter-cluster similarity:
          Average similarity between texts in different clusters (lower values indicate
          better separation between clusters)
        - Silhouette-like score:
          Difference between intra-cluster and inter-cluster similarity (similar to
          silhouette score but calculated differently)

        The method handles edge cases:
        - Returns zeros if there are fewer than 2 samples
        - Uses cosine similarity for text embeddings
        - Only computes the upper triangle of the similarity matrix for efficiency

        Returns:
            dict[str, Union[float, numpy.floating]]: Dictionary with the following keys:
                - intra_cluster_similarity: Average similarity within clusters
                - inter_cluster_similarity: Average similarity between clusters
                - silhouette_like_score: Difference between intra and inter similarity
        """
        try:
            # Skip if we have too few samples
            if len(self.embeddings) < 2:
                logger.warning("Not enough samples to calculate similarity metrics")
                return {
                    "intra_cluster_similarity": 0.0,
                    "inter_cluster_similarity": 0.0,
                    "silhouette_like_score": 0.0,
                }

            # Calculate similarity matrix
            similarity_matrix = cosine_similarity(self.embeddings)

            # Initialize counters and sums
            intra_cluster_sum = 0.0
            intra_cluster_count = 0
            inter_cluster_sum = 0.0
            inter_cluster_count = 0

            # Calculate intra-cluster and inter-cluster similarities
            for i in range(len(self.embeddings)):
                for j in range(i + 1, len(self.embeddings)):  # Only upper triangle
                    if self.cluster_assignments[i] == self.cluster_assignments[j]:
                        # Same cluster (intra-cluster)
                        intra_cluster_sum += similarity_matrix[i, j]
                        intra_cluster_count += 1
                    else:
                        # Different clusters (inter-cluster)
                        inter_cluster_sum += similarity_matrix[i, j]
                        inter_cluster_count += 1

            # Calculate averages
            if intra_cluster_count > 0:
                intra_cluster_similarity = intra_cluster_sum / intra_cluster_count
            else:
                intra_cluster_similarity = 0.0

            if inter_cluster_count > 0:
                inter_cluster_similarity = inter_cluster_sum / inter_cluster_count
            else:
                inter_cluster_similarity = 0.0

            # Calculate a silhouette-like score
            silhouette_like = intra_cluster_similarity - inter_cluster_similarity

            return {
                "intra_cluster_similarity": float(intra_cluster_similarity),
                "inter_cluster_similarity": float(inter_cluster_similarity),
                "silhouette_like_score": float(silhouette_like),
            }

        except Exception as e:
            logger.error(f"Error calculating similarity metrics: {e}")
            return {
                "intra_cluster_similarity": 0.0,
                "inter_cluster_similarity": 0.0,
                "silhouette_like_score": 0.0,
            }

    def detect_powerlaw_distribution(self) -> dict[str, Any]:
        """
        Detect if the cluster size distribution follows a power-law.

        This method analyzes the distribution of cluster sizes to determine if it
        follows a power-law distribution, which is common in many natural language
        datasets and indicates scale-free properties. The analysis includes:

        1. Collecting the size of each cluster
        2. Validating if there are enough clusters (at least 5) for meaningful analysis
        3. Fitting a power-law distribution using the powerlaw package
        4. Comparing the power-law fit to an exponential distribution

        The method handles edge cases:

        - Returns null values if there are fewer than 5 clusters
        - Handles errors in the powerlaw fitting process
        - Validates the fitted parameters to avoid NaN values

        Returns:
            dict[str, Any]: A dictionary with power-law parameters:
                - alpha: Power-law exponent (higher values indicate steeper distribution)
                - xmin: Minimum value for which power-law holds
                - is_powerlaw: Boolean indicating if distribution follows power-law
                - sigma_error: Standard error of the alpha estimate
                - p_value: P-value from comparison with exponential distribution
        """  # noqa: E501
        default_powerlaw_results = {
            "alpha": None,
            "xmin": None,
            "is_powerlaw": False,
            "sigma_error": None,
            "p_value": None,
        }

        try:
            import powerlaw

            # 1. Get cluster sizes
            cluster_sizes = []
            for cluster_id in self.unique_clusters:
                size = self.cluster_assignments.count(cluster_id)
                cluster_sizes.append(size)

            # 2. Check if there are enough clusters and unique sizes for the analysis.
            unique_sizes = set(cluster_sizes)

            if len(cluster_sizes) < 5:
                logger.warning("Not enough clusters to detect power-law distribution")
                return default_powerlaw_results
            elif len(unique_sizes) < 2:
                logger.warning(
                    "Not enough unique cluster sizes to detect power-law distribution"
                )  # noqa: E501
                return default_powerlaw_results

            # 3. Fit power-law distribution
            fit = powerlaw.Fit(cluster_sizes, discrete=True, verbose=False)

            alpha = fit.alpha
            xmin = fit.xmin
            sigma = fit.sigma if hasattr(fit, "sigma") else 0.0

            # Check for NaN values
            if alpha is None or np.isnan(alpha) or xmin is None or np.isnan(xmin):
                logger.warning("Power-law fit returned NaN values")
                return default_powerlaw_results

            # Test if distribution follows power-law
            # Compare to exponential distribution
            try:
                ratio, p_value = fit.distribution_compare(
                    "power_law", "exponential", normalized_ratio=True
                )
                # Positive ratio means power_law is better
                is_powerlaw = ratio > 0 and p_value < 0.1
            except Exception as e:
                logger.error(f"Error comparing distributions: {e}")
                ratio, p_value = None, None
                is_powerlaw = False

            return {
                "alpha": float(alpha),
                "xmin": float(xmin),
                "is_powerlaw": is_powerlaw,
                "sigma_error": (
                    float(sigma) if sigma is not None and not np.isnan(sigma) else None
                ),  # noqa: E501
                "p_value": (
                    float(p_value)
                    if p_value is not None and not np.isnan(p_value)
                    else None
                ),  # noqa: E501
            }
        except Exception as e:
            logger.error(f"Error detecting power-law distribution: {e}")
            return default_powerlaw_results

    def find_outliers(self, n_neighbors: int = 5) -> dict[str, float]:
        """
        Find potential outliers in each cluster using nearest neighbors.

        Args:
            n_neighbors: Number of neighbors to consider (default: 5)

        Returns:
            dict[str, float]: Dictionary with outlier metrics
        """
        try:
            # Skip if we have too few samples
            if len(self.embeddings) < n_neighbors + 1:
                logger.warning("Not enough samples to detect outliers")
                return {}

            # Fit nearest neighbors
            nn = NearestNeighbors(n_neighbors=n_neighbors)
            nn.fit(self.embeddings)

            # Get distances to nearest neighbors
            distances, _ = nn.kneighbors(self.embeddings)

            # Calculate outlier score as mean distance to neighbors
            outlier_scores = distances.mean(axis=1)

            # Create dictionary of outlier scores
            result = {}
            for i, score in enumerate(outlier_scores):
                result[str(i)] = float(score)

            return result

        except Exception as e:
            logger.error(f"Error detecting outliers: {e}")
            return {}

    def _get_cluster_sizes(self) -> dict[str, int]:
        """
        Get the size distribution of clusters.

        Returns:
            dict[str, int]: Dictionary mapping cluster IDs to their sizes
        """
        cluster_sizes = {}
        for cluster_id in self.unique_clusters:
            cluster_sizes[str(cluster_id)] = self.cluster_assignments.count(cluster_id)
        return cluster_sizes

    def generate_report(self) -> "dict[str, Any]":
        """
        Generate a comprehensive evaluation report.

        Returns:
            dict[str, Any]: Dictionary containing all evaluation metrics and metadata
        """
        # Calculate all metrics
        silhouette = self.calculate_silhouette_score()
        similarity_metrics = self.calculate_similarity_metrics()
        powerlaw_metrics = self.detect_powerlaw_distribution()
        outliers = self.find_outliers()

        # Compile the report
        report = {
            "model_name": self.model_name,
            "parameters": {
                "alpha": self.alpha,
                "sigma": self.sigma,
                "variance": self.variance,
                "random_state": self.random_state,
            },
            "cluster_stats": {
                "num_clusters": len(self.unique_clusters),
                "num_texts": len(self.texts),
                "cluster_sizes": self._get_cluster_sizes(),
            },
            "metrics": {
                "silhouette_score": silhouette,
                "similarity": similarity_metrics,
                "powerlaw": powerlaw_metrics,
                "outliers": outliers,
            },
        }

        return report


def _sanitize_for_json(obj):
    """Convert NumPy types to Python types for JSON serialization."""
    if isinstance(obj, dict):
        return {k: _sanitize_for_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_sanitize_for_json(item) for item in obj]
    elif isinstance(obj, (np.integer, np.floating, np.bool_)):  # type: ignore
        return obj.item()
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    else:
        return obj


def _debug_json_error(report: dict[str, Any]) -> None:
    """Debug JSON serialization errors by identifying problematic values."""
    for model_name, model_report in report.items():
        try:
            json.dumps(model_report, cls=NumpyEncoder)
        except TypeError:
            logger.error(f"Problem in model report: {model_name}")

            for key, value in model_report.items():
                try:
                    json.dumps({key: value}, cls=NumpyEncoder)
                except TypeError:
                    logger.error(f"Problem with key: {key}, value type: {type(value)}")


def _create_simplified_report(report: dict[str, Any]) -> dict[str, Any]:
    """Create a simplified version of the report with only basic metrics."""
    simplified_report = {}
    for model_name, model_report in report.items():
        simplified_report[model_name] = {
            "basic_metrics": model_report.get("basic_metrics", {}),
            "silhouette_score": model_report.get("silhouette_score", 0.0),
        }
    return simplified_report


def save_evaluation_report(
    report: dict[str, Any],
    output_dir: str,
    filename: str = "evaluation_report.json",
) -> str:
    """
    Save the evaluation report to a JSON file.

    This function serializes the evaluation report to a JSON file, handling NumPy
    data types through the NumpyEncoder. The report contains comprehensive metrics
    about the clustering quality, including silhouette scores, similarity metrics,
    power-law analysis, and outlier detection.

    If serialization issues occur, the function attempts to save a simplified version
    of the report with only basic metrics.

    Args:
        report: Dictionary containing the evaluation report for different clustering
            models
        output_dir: Directory to save the report
        filename: Name of the output file (default: "evaluation_report.json")

    Returns:
        str: Path to the saved report file

    Raises:
        TypeError: If JSON serialization fails even after simplification attempts
    """
    output_path = os.path.join(output_dir, filename)

    try:
        # Sanitize the report
        sanitized_report = _sanitize_for_json(report)

        with open(output_path, "w") as f:
            json.dump(sanitized_report, f, indent=2, cls=NumpyEncoder)

        logger.info(f"Evaluation report saved to {output_path}")
        return output_path
    except TypeError as e:
        # If we still have serialization issues, log detailed information
        logger.error(f"JSON serialization error: {e}")

        # Debug the error
        _debug_json_error(report)

        # Save a simplified version
        simplified_report = _create_simplified_report(report)
        with open(output_path, "w") as f:
            json.dump(simplified_report, f, indent=2)

        logger.warning(
            f"Saved simplified report to {output_path} due to serialization issues"
        )
        return output_path
