"""
Evaluation module for QA Dataset Clustering.

This module provides tools for evaluating the quality and consistency of clusters
generated by the clustering algorithms. It implements established metrics for
cluster validation in the context of text data clustering.

"""

import json
import os
from typing import Any, Dict, List, Union

import numpy as np
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors

from .logging import get_logger

logger = get_logger(__name__)


# Custom JSON encoder for NumPy types
class NumpyEncoder(json.JSONEncoder):
    """
    Custom JSON encoder that handles NumPy data types.

    This encoder converts NumPy types to their Python equivalents:

    - :class:`numpy.ndarray` -> :class:`list`
    - :class:`numpy.single`, :class:`numpy.double` -> :class:`float`
    - :class:`numpy.intc`, :class:`numpy.int_` -> :class:`int`
    - :class:`numpy.bool` -> :class:`bool`
    """

    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.single, np.double)):  # type: ignore
            return float(obj)
        if isinstance(obj, (np.intc, np.int_)):  # type: ignore
            return int(obj)
        if isinstance(obj, np.bool):  # type: ignore
            return bool(obj)
        if isinstance(obj, bool):
            return bool(obj)
        try:
            # Try to convert any other NumPy type to a Python type
            return obj.item() if hasattr(obj, "item") else obj
        except (AttributeError, ValueError, TypeError):
            return super().default(obj)


class ClusterEvaluator:
    """
    Evaluates the quality of text clusters using established metrics.

    This class provides methods to assess cluster quality using metrics like
    silhouette score, which measures how similar an object is to its own cluster
    compared to other clusters.

    Note on parameters:

    - alpha, sigma: Input parameters used in the clustering algorithms
      (Dirichlet Process and Pitman-Yor Process)
    - variance: Variance parameter for the likelihood model in clustering
    - random_state: Random seed for reproducible evaluation

    - In detect_powerlaw_distribution():

      - alpha: Output parameter representing the power law exponent
      - sigma_error: Standard error of the power law alpha estimate

    These parameters share names but represent different concepts.
    """

    def __init__(
        self,
        texts: List[str],
        embeddings: np.ndarray,
        cluster_assignments: List[int],
        model_name: str,
        alpha: float = 1.0,
        sigma: float = 0.0,
        variance: float = 0.1,
        random_state: Union[int, None] = None,
    ):
        """
        Initialize the cluster evaluator.

        Args:
            texts: List of text strings that were clustered
            embeddings: Numpy array of embeddings for each text
            cluster_assignments: List of cluster IDs for each text
            model_name: Name of the clustering model (e.g., "DP", "PYP")
            alpha: Concentration parameter (default: 1.0)
            sigma: Discount parameter for Pitman-Yor Process (default: 0.0)
            variance: Variance parameter for likelihood model (default: 0.1)
            random_state: Random seed for reproducible evaluation (default: None)
        """
        self.texts = texts
        self.embeddings = embeddings
        self.cluster_assignments = cluster_assignments
        self.model_name = model_name
        self.alpha = alpha
        self.sigma = sigma
        self.variance = variance
        self.random_state = random_state
        self.unique_clusters = sorted(set(cluster_assignments))

        # Set random state for reproducibility if provided
        if random_state is not None:
            np.random.seed(random_state)

        # Validate inputs
        if len(texts) != len(embeddings) or len(texts) != len(cluster_assignments):
            raise ValueError(
                "Length mismatch: texts, embeddings, and cluster_assignments "
                "must have the same length"
            )

        logger.info(
            f"Initialized cluster evaluator for {model_name} with {len(texts)} texts "
            f"and {len(self.unique_clusters)} clusters"
        )

    def calculate_silhouette_score(self) -> Union[float, int]:
        """
        Calculate the silhouette score for the clustering.

        The silhouette score measures how similar an object is to its own cluster
        compared to other clusters. The score ranges from -1 to 1, where:

        - A high value (close to 1) indicates the object is well matched to its cluster
        - A value near 0 indicates the object is on or very close to the decision
          boundary
        - A negative value indicates the object might be assigned to the wrong cluster

        Returns:
            Silhouette score as a float
        """
        # We need at least 2 clusters and each cluster must have at least 2 samples
        if len(self.unique_clusters) < 2:
            logger.warning(
                f"Cannot calculate silhouette score: only "
                f"{len(self.unique_clusters)} cluster found"
            )
            return 0.0

        # Count samples per cluster
        cluster_counts = {}
        for cluster_id in self.cluster_assignments:
            cluster_counts[cluster_id] = cluster_counts.get(cluster_id, 0) + 1

        # Check if any cluster has only one sample
        single_sample_clusters = [c for c, count in cluster_counts.items() if count < 2]
        if single_sample_clusters:
            logger.warning(
                f"Cannot calculate silhouette score: "
                f"{len(single_sample_clusters)} clusters have fewer than 2 samples"
            )
            return 0.0

        try:
            score = silhouette_score(
                self.embeddings, self.cluster_assignments, metric="cosine"
            )
            logger.info(f"Silhouette score for {self.model_name}: {score:.4f}")
            return float(score)
        except Exception as e:
            logger.error(f"Error calculating silhouette score: {e}")
            return 0.0

    def calculate_similarity_metrics(self) -> Dict[str, Union[float, np.floating]]:
        """
        Calculate similarity metrics for the clusters.

        This method computes:

        - Intra-cluster similarity:
          Average similarity between texts in the same cluster
        - Inter-cluster similarity:
          Average similarity between texts in different clusters

        Returns:
            Dictionary with similarity metrics
        """
        try:
            # Skip if we have too few samples
            if len(self.embeddings) < 2:
                logger.warning("Not enough samples to calculate similarity metrics")
                return {
                    "intra_cluster_similarity": 0.0,
                    "inter_cluster_similarity": 0.0,
                    "silhouette_like_score": 0.0,
                }

            # Calculate similarity matrix
            similarity_matrix = cosine_similarity(self.embeddings)

            # Initialize counters and sums
            intra_cluster_sum = 0.0
            intra_cluster_count = 0
            inter_cluster_sum = 0.0
            inter_cluster_count = 0

            # Calculate intra-cluster and inter-cluster similarities
            for i in range(len(self.embeddings)):
                for j in range(i + 1, len(self.embeddings)):  # Only upper triangle
                    if self.cluster_assignments[i] == self.cluster_assignments[j]:
                        # Same cluster (intra-cluster)
                        intra_cluster_sum += similarity_matrix[i, j]
                        intra_cluster_count += 1
                    else:
                        # Different clusters (inter-cluster)
                        inter_cluster_sum += similarity_matrix[i, j]
                        inter_cluster_count += 1

            # Calculate averages
            if intra_cluster_count > 0:
                intra_cluster_similarity = intra_cluster_sum / intra_cluster_count
            else:
                intra_cluster_similarity = 0.0

            if inter_cluster_count > 0:
                inter_cluster_similarity = inter_cluster_sum / inter_cluster_count
            else:
                inter_cluster_similarity = 0.0

            # Calculate a silhouette-like score
            silhouette_like = intra_cluster_similarity - inter_cluster_similarity

            return {
                "intra_cluster_similarity": float(intra_cluster_similarity),
                "inter_cluster_similarity": float(inter_cluster_similarity),
                "silhouette_like_score": float(silhouette_like),
            }

        except Exception as e:
            logger.error(f"Error calculating similarity metrics: {e}")
            return {
                "intra_cluster_similarity": 0.0,
                "inter_cluster_similarity": 0.0,
                "silhouette_like_score": 0.0,
            }

    def detect_powerlaw_distribution(self) -> Dict[str, Any]:
        """
        Detect if the cluster size distribution follows a power-law.

        Returns:
            typing.Dict: A dictionary with power-law parameters:
                - alpha: Power-law exponent
                - xmin: Minimum value for which power-law holds
                - is_powerlaw: Whether the distribution follows a power-law
                - sigma_error: Standard error of the alpha estimate
        """
        try:
            # Get cluster sizes
            cluster_sizes = []
            for cluster_id in self.unique_clusters:
                size = self.cluster_assignments.count(cluster_id)
                cluster_sizes.append(size)

            # Need at least a few clusters to detect power-law
            if len(cluster_sizes) < 5:
                logger.warning("Not enough clusters to detect power-law distribution")
                return {
                    "alpha": None,
                    "xmin": None,
                    "is_powerlaw": False,
                    "sigma_error": None,
                    "p_value": None,
                }

            # Try to import powerlaw package
            try:
                import powerlaw
            except ImportError:
                logger.warning(
                    "powerlaw package not installed, cannot detect power-law"
                )
                return {
                    "alpha": None,
                    "xmin": None,
                    "is_powerlaw": False,
                    "sigma_error": None,
                    "p_value": None,
                }

            # Fit power-law to cluster size distribution
            fit = powerlaw.Fit(cluster_sizes, discrete=True)

            # Get power-law parameters
            alpha = fit.alpha
            xmin = fit.xmin
            sigma = fit.sigma if hasattr(fit, "sigma") else 0.0

            # Test if distribution follows power-law
            # Compare to exponential distribution
            R, p = fit.distribution_compare(
                "power_law", "exponential", normalized_ratio=True
            )
            is_powerlaw = R > 0 and p < 0.1  # Positive R means power_law is better

            return {
                "alpha": float(alpha) if alpha is not None else None,
                "xmin": float(xmin) if xmin is not None else None,
                "is_powerlaw": bool(is_powerlaw),
                "sigma_error": float(sigma) if sigma is not None else None,
                "p_value": float(p) if p is not None else None,
            }

        except Exception as e:
            logger.error(f"Error detecting power-law distribution: {e}")
            return {
                "alpha": None,
                "xmin": None,
                "is_powerlaw": False,
                "sigma_error": None,
                "p_value": None,
            }

    def find_outliers(self, n_neighbors: int = 5) -> Dict[str, float]:
        """
        Find potential outliers in each cluster using nearest neighbors.

        Args:
            n_neighbors: Number of neighbors to consider (default: 5)

        Returns:
            Dictionary with outlier metrics
        """
        try:
            # Skip if we have too few samples
            if len(self.embeddings) < n_neighbors + 1:
                logger.warning("Not enough samples to detect outliers")
                return {}

            # Fit nearest neighbors
            nn = NearestNeighbors(n_neighbors=n_neighbors)
            nn.fit(self.embeddings)

            # Get distances to nearest neighbors
            distances, _ = nn.kneighbors(self.embeddings)

            # Calculate outlier score as mean distance to neighbors
            outlier_scores = distances.mean(axis=1)

            # Create dictionary of outlier scores
            result = {}
            for i, score in enumerate(outlier_scores):
                result[str(i)] = float(score)

            return result

        except Exception as e:
            logger.error(f"Error detecting outliers: {e}")
            return {}

    def _get_cluster_sizes(self) -> Dict[str, int]:
        """
        Get the size distribution of clusters.

        Returns:
            Dictionary mapping cluster IDs to their sizes
        """
        cluster_sizes = {}
        for cluster_id in self.unique_clusters:
            cluster_sizes[str(cluster_id)] = self.cluster_assignments.count(cluster_id)
        return cluster_sizes

    def generate_report(self) -> Dict[str, Any]:
        """
        Generate a comprehensive evaluation report.

        Returns:
            Dictionary containing all evaluation metrics and metadata
        """
        # Calculate all metrics
        silhouette = self.calculate_silhouette_score()
        similarity_metrics = self.calculate_similarity_metrics()
        powerlaw_metrics = self.detect_powerlaw_distribution()
        outliers = self.find_outliers()

        # Compile the report
        report = {
            "model_name": self.model_name,
            "parameters": {
                "alpha": self.alpha,
                "sigma": self.sigma,
                "variance": self.variance,
                "random_state": self.random_state,
            },
            "cluster_stats": {
                "num_clusters": len(self.unique_clusters),
                "num_texts": len(self.texts),
                "cluster_sizes": self._get_cluster_sizes(),
            },
            "metrics": {
                "silhouette_score": silhouette,
                "similarity": similarity_metrics,
                "powerlaw": powerlaw_metrics,
                "outliers": outliers,
            },
        }

        return report


def _sanitize_for_json(obj):
    """Convert NumPy types to Python types for JSON serialization."""
    if isinstance(obj, dict):
        return {k: _sanitize_for_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_sanitize_for_json(item) for item in obj]
    elif isinstance(obj, (np.integer, np.floating, np.bool_)):  # type: ignore
        return obj.item()
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    else:
        return obj


def _debug_json_error(report):
    """Debug JSON serialization errors by identifying problematic values."""
    for model_name, model_report in report.items():
        try:
            json.dumps(model_report, cls=NumpyEncoder)
        except TypeError:
            logger.error(f"Problem in model report: {model_name}")

            for key, value in model_report.items():
                try:
                    json.dumps({key: value}, cls=NumpyEncoder)
                except TypeError:
                    logger.error(f"Problem with key: {key}, value type: {type(value)}")


def _create_simplified_report(report):
    """Create a simplified version of the report with only basic metrics."""
    simplified_report = {}
    for model_name, model_report in report.items():
        simplified_report[model_name] = {
            "basic_metrics": model_report.get("basic_metrics", {}),
            "silhouette_score": model_report.get("silhouette_score", 0.0),
        }
    return simplified_report


def save_evaluation_report(
    report: Dict[str, Any], output_dir: str, filename: str = "evaluation_report.json"
) -> str:
    """
    Save the evaluation report to a JSON file.

    Args:
        report: Dictionary containing the evaluation report
        output_dir: Directory to save the report
        filename: Name of the output file

    Returns:
        Path to the saved report file
    """
    output_path = os.path.join(output_dir, filename)

    try:
        # Sanitize the report
        sanitized_report = _sanitize_for_json(report)

        with open(output_path, "w") as f:
            json.dump(sanitized_report, f, indent=2, cls=NumpyEncoder)

        logger.info(f"Evaluation report saved to {output_path}")
        return output_path
    except TypeError as e:
        # If we still have serialization issues, log detailed information
        logger.error(f"JSON serialization error: {e}")

        # Debug the error
        _debug_json_error(report)

        # Save a simplified version
        simplified_report = _create_simplified_report(report)
        with open(output_path, "w") as f:
            json.dump(simplified_report, f, indent=2)

        logger.warning(
            f"Saved simplified report to {output_path} due to serialization issues"
        )
        return output_path
